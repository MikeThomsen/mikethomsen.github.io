---
title: A data engineering perspective on MongoDB
---

I came late to the game with MongoDB, having my first real professional experience with it in 2017 and doing a lot of work with refactoring the Apache NiFi support for it in 2017 through 2018. Over the years, I had seen lots of people attacking it and telling me why you should never use it under any circumstances. We all know that [it's webscale](https://www.youtube.com/watch?v=b2F-DItXtZs) and [scales almost as well](https://tech.wayfair.com/2013/04/devnull-vs-mongodb-benchmark-bake-off/) as a properly sharded and replicated `/dev/null`, but is it really that bad? Not at all.

MongoDB is a victim of its own marketing materials and success. While Mongo does have some concept of schema validation, at no point do they encourage users to start thinking about it as Agile necessitates refactoring and pivoting on their application--much less when they go to integration testing or production. This effectively means that where a lot of NoSQL data stores are schema-on-read or schema-on-write, MongoDB really doesn't have a schema at all. If some malevolent jerk on your team decides to seed a collection with a Date in the `phoneNumber` field, MongoDB will happily accept that even if everything prior was a string or complex object.

Yet here I'm not willing to give the SQL camp credit for the observation that Mongo users either have a formal schema or a de facto schema because there is a third option: simply chaos. In the case of Mongo, virtually any change is valid to the database when it is used this way. That means there isn't a de facto one unless you mean one in the sense of an abstract metaphysical, objective representation of the current state of the system that might not actually exist fully in the head of any one member of the team. One example of this is that I have seen dates in a single collection represented as long integers, Date objects, and probably about half a dozen distinctly different string representation (and half of them weren't a flavor of ISO8601 either). Just think about trying to query on that and explaining to MongoDB that no really, they're all "dates."

The logic there from a typical full stack developer was probably "we'll figure this out later" and later ended up coming, as it always does. Eventually the ease of not having to make an ETL process that can reconcile all of the ways the dirty data being sent to you represents dates becomes your headache or perhaps the headache of your DBA if you have one. Either your database slaps you on the hand and hisses at you for attempting to send it obviously wrong input or it stabs you in the back when you expect it to give you good results. There is no magic here, and as always there ain't no such thing as a free lunch.

Besides [FizzBuzz](https://blog.codinghorror.com/why-cant-programmers-program/) probably the best way to weed out **bad** developers is to just get them talking about data management. That is modeling, cleanup, migration, etc. I haven't tried it much, but my working suspicion is that you will find a strong correlation between developers who don't put much attention to that with developers who don't put proper attention to testing, clean design and a host of other things. Data-related tasks are the boring *sine qua non* of writing even a slightly complex application, and attention to detail here is a bellweather for the rest of their contributions.

If you're a MongoDB developer who scoffs at any of this, I invite you to read [this article](https://foone.wordpress.com/2019/02/14/normalization-of-deviance/). If you need a TL;DR many systems that "seem to work" are merely lucky. If you don't take serious steps to ensure good data practices because you like to "move fast and break things," you almost assuredly will end up breaking something. That something might also be your applciation and the time it happens is when you're trying to impress a client, customer, VC, whatever.

For those who use MongoDB and NiFi, the Record API will be an invaluable friend in a lot of cases. Avro is very flexible and easy going compared to a SQL schema. If you define a fiend as a long and someone supplies a string, Avro will kindly parse the string into a long. Nice things like that. If a team cannot agree to a data model and represent it with something as simple as Avro (or Thrift), then the team needs to be prepared to work in "interesting times."